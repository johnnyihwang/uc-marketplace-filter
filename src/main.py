import random
import re
import json
import time
import argparse
import getpass
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.firefox.options import Options
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
from PyQt5 import QtCore, QtDBus, QtWidgets

def fb_login(credentials):
    """
    Returns the selenium driver (as closed) and a dict of FB login cookies 
    """

    options = Options()
    options.headless = True
    driver = webdriver.Firefox(options=options)
    driver.get('https://www.facebook.com/')
    driver.implicitly_wait(0)

    # Attempt login
    email_xpath = '//*[@id="email"]'
    pass_xpath = '//*[@id="pass"]'
    login_xpath = '//*[@data-testid="royal_login_button"]'
    searchbar_xpath = '//input[@name="q" and @data-testid="search_input" and @aria-label="Search"]'

    # Log in
    driver.find_element_by_xpath(email_xpath).send_keys(credentials['email'])
    driver.find_element_by_xpath(pass_xpath).send_keys(credentials['pass'])
    driver.find_element_by_xpath(login_xpath).click()

    # Wait until search bar is located
    try :
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, searchbar_xpath)))
    except TimeoutException :
        print("Could not login!")
        exit(1)

    # Storing the cookies generated by the browser
    driver_cookies = driver.get_cookies()
    driver.get("about:blank")
    return (driver, driver_cookies)



def get_bs(driver, cookies, url):
    """
    Given a selenium driver and cookie (as dict), returns a BeautifulSoup object.
    """
    driver.get(url)

    # Wait for page to load
    try :
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, 'pagelet_group_mall')))
        # Press end 5 times
        driver.implicitly_wait(5)
        for i in range(6) :
            body = driver.find_element_by_css_selector('body')
            print("Pressing end")
            body.send_keys(Keys.END)
            time.sleep(5)
    except TimeoutException :
        return None
    driver.implicitly_wait(0)
    bs = BeautifulSoup(driver.page_source, 'lxml')
    return bs



def parse_post(bs, keywords) :
    """
    Given a post (wrapped in userContentWrapper), parses the contents and the title
    """
    # Post Title
    try: 
        post_titles = bs.find("div", {"class":"mtm"}).find("a", {"class":"_8_4r"})
        if post_titles is None:
            print("\t No title found!")
        else :
            # TODO: manually traversing to find child, might not work if site changes
            for i in range(4) :
                post_titles = next(post_titles.children)
            post_titles = post_titles.next_sibling
            post_title = post_titles.find("span", {"class":False})
            if post_title is None :
                print("\t No title found!")
            else :
                if keyword_match(" ".join(list(post_titles.stripped_strings)), keywords) :
                    return True
    except AttributeError :
        print("\t AttributeError when trying to find titles: likely titles don't exist")


    # Post Content
    post_data = bs.find("div", {"class":re.compile("userContent")})
    if post_data is None:
        print("No user post content...try checking manually")
        return False

    paragraphs = []
    for p in post_data.find_all("p") :
        paragraphs.extend(list(p.stripped_strings))
    if keyword_match(" ".join(paragraphs), keywords) :
        return True

    # Hidden Texts
    # This should already be handled by the above case, but check just in case
    paragraphs = []
    for hidden in post_data.find_all("div", {"class":"text_exposed_show"}) :
        for p in hidden.find_all("p") :
            paragraphs.extend(list(p.stripped_strings))
    if keyword_match(" ".join(paragraphs), keywords):
        return True
    
    return False



def crawl_group(group_id, driver, cookie, post_limit, keywords, processed_post, notify):
    """
    Goes to group URL, crawls it and extracts posts URLs.
    """
    # Note: sale post id is groups/(group id)/permalink/(sale post id)
    
    group_url = "https://www.facebook.com/groups/"+str(group_id)+"/?sorting_setting=RECENT_ACTIVITY"
    group_bs = get_bs(driver, cookie, group_url)

    # finding pagelet div
    group_mall = group_bs.find(id='pagelet_group_mall')
    user_wrapper = group_mall.find_all("div", {"class":re.compile("userContentWrapper")})

    # Only check the top (post_limit) posts
    for i in range(min(post_limit, len(user_wrapper))) :
        # Checking post
        id = id_atoi(str(user_wrapper[i].parent.parent['id']))
        if id in processed_post :
            continue
        else :
            processed_post.add(id)
            print("==== New Post - ID {} ====".format(id))
            if parse_post(user_wrapper[i], keywords) :
                alert(i, group_id, id, notify)
            print("")


"""
Helper functions
"""

def id_atoi(string):
    (start, j) = (0, 0)
    for i in range(len(string)):
        if string[i].isdigit():
            start = i
            j = i
            while j < len(string) and string[j].isdigit():
                j += 1
            break
    return int(string[start:j])

def json_to_obj(filename):
    """
    Extracts data from JSON file and saves it on Python object
    """
    obj = None
    with open(filename) as json_file:
        obj = json.loads(json_file.read())
    return obj

def read_keywords(filename):
    """
    Reads keywords file and makes list of regex patterns
    """
    keywords = []
    with open(filename, "r") as f:
        for word in f :
            keyword = []
            for char in word :
                # Skip whitespace characters
                if char.isspace() :
                    continue
                keyword.append(char)
                keyword.append(r"\s*")
            keywords.append("".join(keyword))
    return keywords

def keyword_match(string, keywords) :
    for keyword in keywords :
        if re.search(keyword, string, re.IGNORECASE) is not None :
            return True
    return False

def alert(i, group_id, id, notify) :
    print("######### Match found at {0}th post! Link: {1} ###########".format(i, text))
    if not notify :
        return
    
    item = "org.freedesktop.Notifications"
    path = "/org/freedesktop/Notifications"
    interface = "org.freedesktop.Notifications"
    app_name = "Marketplace Notification"
    v = QtCore.QVariant(91424)
    if v.convert(QtCore.QVariant.UInt):
        id_replace = v
    icon = ""
    title = "Match found in marketplace!"
    text = "https://www.facebook.com/groups/{0}/permalink/{1}".format(group_id, id)
    actions_list = QtDBus.QDBusArgument([], QtCore.QMetaType.QStringList)
    hint = {}
    time = 7000   # milliseconds for display timeout

    bus = QtDBus.QDBusConnection.sessionBus()
    if not bus.isConnected():
        print("Not connected to dbus!")
    notify = QtDBus.QDBusInterface(item, path, interface, bus)
    if notify.isValid():
        x = notify.call(QtDBus.QDBus.AutoDetect, "Notify", app_name,
                        id_replace, icon, title, text,
                        actions_list, hint, time)
        if x.errorName():
            print("Failed to send notification!")
            print(x.errorMessage())
    else:
        print("Invalid dbus interface")

"""
Main function
"""

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Web scraper to detect if new posts on Facebook group matches given keywords.')
    parser.add_argument('-k', '--keyword', nargs='?', default="keywords", type=str,
                        help='The filename of the list of keywords',
                        metavar='FILENAME', dest='keyword')
    parser.add_argument('-c', '--credentials', nargs='?', const="credentials.json", default=None, type=str,
                        help='The file name of the credentials, stdin used if flag not used. Provide a JSON file.',
                        metavar='FILENAME', dest='credentials')
    parser.add_argument('-g', '--group-id', nargs='?', default=279055445511955, type=int,
                        help='The ID of the Facebook group',
                        metavar='ID', dest='groupid')
    parser.add_argument('-n',  '--no-notification', action='store_false', help='Turns Qt notification off', dest='notify')
    
    parser.add_argument('-p', '--poll-time', nargs='?', default=270, type=int,
                        help='The number of seconds to wait before polling - actual time between polling will be between given seconds and given seconds + 50',
                        metavar='SECONDS', dest='poll_time')
    
    args = parser.parse_args()
    
    # Extracts credentials for the login and all of the profiles URL to scrape
    if args.credentials is None:
        email = input("Your Facebook email: ")
        pwd = getpass.getpass(prompt="Your Facebook password: ")
        credentials = {'email':email, 'pass':pwd}
    else :
        credentials = json_to_obj(args.credentials)

    # Get list of keywords
    keywords = read_keywords(args.keyword)

    # Attempt login
    print("Logging in...")
    (driver, cookies) = fb_login(credentials)
    print("Log in complete")
    
    # Run loop
    processed_post = set()
    group_id = args.groupid
    while(True) :
        print("================================================")
        print("==== Updating at " + datetime.now().strftime("%Y-%b-%d (%a) %H:%M:%S" + " ===="))
        print("================================================")
        crawl_group(group_id, driver, cookies, 30, keywords, processed_post, args.notify)
        time_limit = args.poll_time + random.randint(0,50)
        for i in range(time_limit, 0, -1) :
            print("Wait {} seconds...".format(i), end="\r")
            time.sleep(1)
